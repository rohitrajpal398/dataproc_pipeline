name: Deploy Dataproc Job and DAG

on:
  push:
    branches: [ main ]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.8'

      - name: Write GCP credentials to file
        run: |
          echo '${{ secrets.GCP_SA_KEY }}' > ${{ github.workspace }}/gcloud-key.json

      - name: Authenticate with Google Cloud
        run: |
          gcloud auth activate-service-account --key-file=${{ github.workspace }}/gcloud-key.json
          gcloud config set project airy-actor-457907-a8
        env:
          GOOGLE_APPLICATION_CREDENTIALS: ${{ github.workspace }}/gcloud-key.json

      - name: Set up gcloud CLI
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: airy-actor-457907-a8
          install_components: 'gsutil'

      - name: Upload Spark Job to GCS
        run: |
          gsutil cp dataproc_pipeline_project/spark_jobs/main_job.py gs://sample-data-for-pract1/spark_jobs/

      - name: Upload Airflow DAG to Composer Environment
        run: |
          gsutil cp dataproc_pipeline_project/dags/dataproc_job_dag.py gs://us-central1-sanbox-env-e1b4197d-bucket/dags/
